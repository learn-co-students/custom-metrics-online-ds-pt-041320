{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Metric Example\n",
    "\n",
    "Let's make a custom metric and use it with cross-validation and `GridSearchCV`\n",
    "\n",
    "For the sake of simplicity, let's use the iris toy dataset\n",
    "\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data.data, columns=data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.Series(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    50\n",
       "1    50\n",
       "0    50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "For logistic regression to converge, let's scale the data\n",
    "\n",
    "(I didn't do this in my initial example and it created a lot of red warnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Modeling with Logistic Regression\n",
    "\n",
    "We aren't doing a train-test split since this is a tiny dataset and it's difficult enough to get a recall score below 1.0. In any real modeling situation you would want to do a train-test split!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=2020, solver=\"liblinear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=2020, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation with Already-Existing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9266666666666666"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50,  0,  0],\n",
       "       [ 0, 43,  7],\n",
       "       [ 0,  4, 46]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y, lr.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(lr, X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9266666666666666"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y, lr.predict(X), average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation with a Custom Metric\n",
    "\n",
    "Let's say we want the recall score `true_positives / (true_positives + false_negatives)`\n",
    "\n",
    "### Basic Score Calculation\n",
    "\n",
    "We could calculate it by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "43 / (43 + 7 + 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But a more robust technique would be to reuse the `recall_score` method\n",
    "\n",
    "First, note that if we set `average=None`, it gives us an array of recall scores, one for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.  , 0.86, 0.92])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y, lr.predict(X), average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply take the value at a given index, to get the recall score for that class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y, lr.predict(X), average=None)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "We know that a cross-validated score will represent generalization better...how to do that with our custom code?\n",
    "\n",
    "First, put the above code into a function (with the index hard-coded):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_1_recall(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, average=None)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_1_recall(y, lr.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use `make_scorer` to pass your custom function into `cross_val_score`\n",
    "\n",
    "(`make_scorer` takes 3 arguments, and actually calls the `.predict` method on the estimator. In theory we could do this without `make_scorer`, but there would be a higher possibility of making mistakes in our code, so let's just allow `make_scorer` to do the work for us.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6, 0.9, 0.8, 0.7, 1. ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_scores = cross_val_score(lr, X, y, scoring=make_scorer(class_1_recall))\n",
    "recall_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(recall_scores)/len(recall_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this tells us that our generalized class 1 recall score could be as low as 0.6, as high as 1.0.  The average of our cross-validated scores is a bit worse than the score for the dataset overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "Ok, rather than just evaluating a given model's performance, let's try using this custom metric in a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"C\": [10.0, 1.0, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lr = LogisticRegression(random_state=2020, solver=\"liblinear\")\n",
    "grid = GridSearchCV(grid_lr, param_grid, make_scorer(class_1_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=2020, solver='liblinear',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'C': [10.0, 1.0, 0.1], 'penalty': ['l1', 'l2']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(class_1_recall), verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
       "                   random_state=2020, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Evaluation\n",
    "\n",
    "Comparing the grid's best model's performance to our original model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_1_recall(y, grid.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_1_recall(y, lr.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50,  0,  0],\n",
       "       [ 0, 47,  3],\n",
       "       [ 0,  2, 48]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y, grid.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50,  0,  0],\n",
       "       [ 0, 43,  7],\n",
       "       [ 0,  4, 46]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y, lr.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Here we used a toy dataset, the iris dataset, to create an example of a custom recall metric that can be used to:\n",
    "\n",
    " - Describe the model's performance on the training dataset (full data or cross-validated)\n",
    " - Perform a grid search to find hyperparameters optimized for the custom metric, rather than one of the metrics built in to SciKit-Learn\n",
    "\n",
    "Note that this example does not follow machine learning best practices; it is designed to demonstrate a custom metric with minimal code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (prework-labs)",
   "language": "python",
   "name": "prework-labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
